{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55341c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iris ===\n",
      "Classic ANN: 86.00% ± 4.35\n",
      "ONN_Bio:    88.67% ± 5.58\n",
      "P-value: 0.3739, Cohen's d(z): 0.447, 95% CI (ONN-ANN): [-4.74, 10.07]\n",
      "\n",
      "=== Breast Cancer ===\n",
      "Classic ANN: 95.97% ± 1.36\n",
      "ONN_Bio:    96.28% ± 0.81\n",
      "P-value: 0.6440, Cohen's d(z): 0.223, 95% CI (ONN-ANN): [-1.43, 2.05]\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1903852.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 145603.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1587903.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 821568.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "\n",
      "=== MNIST ===\n",
      "Classic ANN: 97.79% ± 0.07\n",
      "ONN_Bio:    97.91% ± 0.15\n",
      "P-value: 0.3619, Cohen's d(z): 0.677, 95% CI (ONN-ANN): [-0.33, 0.59]\n"
     ]
    }
   ],
   "source": [
    "import os, math, time, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import ttest_rel, t\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# ===== MNIST için =====\n",
    "try:\n",
    "    from torchvision import datasets, transforms\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"torchvision gerekli. Lütfen: pip install torchvision\") from e\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Yardımcılar\n",
    "# =========================================\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def accuracy_from_logits(logits, y_true):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y_true).float().mean().item()\n",
    "\n",
    "def paired_stats(a: List[float], b: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"a: ANN skorları, b: ONN skorları; her ikisi de aynı koşuların sonuçları\"\"\"\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    d = b - a\n",
    "    n = len(d)\n",
    "    mean_a = a.mean(); std_a = a.std(ddof=1)\n",
    "    mean_b = b.mean(); std_b = b.std(ddof=1)\n",
    "    mean_diff = d.mean()\n",
    "    sd_diff = d.std(ddof=1) if n > 1 else 0.0\n",
    "\n",
    "    # paired t-test\n",
    "    if n > 1 and sd_diff > 0:\n",
    "        t_stat, p_val = ttest_rel(b, a)\n",
    "        se = sd_diff / math.sqrt(n)\n",
    "        tcrit = t.ppf(0.975, df=n-1)\n",
    "        ci_low = mean_diff - tcrit * se\n",
    "        ci_high = mean_diff + tcrit * se\n",
    "        # Cohen's dz (eşleşik etki büyüklüğü)\n",
    "        cohend = mean_diff / sd_diff\n",
    "    else:\n",
    "        t_stat, p_val, cohend, ci_low, ci_high = float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"mean_a\": mean_a, \"std_a\": std_a,\n",
    "        \"mean_b\": mean_b, \"std_b\": std_b,\n",
    "        \"t\": t_stat, \"p\": p_val, \"dz\": cohend,\n",
    "        \"ci_low\": ci_low, \"ci_high\": ci_high\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Veri setleri: Iris, Breast Cancer, MNIST\n",
    "# =========================================\n",
    "def load_iris_torch():\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    # train/val/test = 60/20/20\n",
    "    n = len(X)\n",
    "    i1 = int(0.6 * n); i2 = int(0.8 * n)\n",
    "    return (TensorDataset(X[:i1], y[:i1]),\n",
    "            TensorDataset(X[i1:i2], y[i1:i2]),\n",
    "            TensorDataset(X[i2:], y[i2:])), 4, 3\n",
    "\n",
    "def load_breast_cancer_torch():\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    n = len(X)\n",
    "    i1 = int(0.6 * n); i2 = int(0.8 * n)\n",
    "    return (TensorDataset(X[:i1], y[:i1]),\n",
    "            TensorDataset(X[i1:i2], y[i1:i2]),\n",
    "            TensorDataset(X[i2:], y[i2:])), X.shape[1], 2\n",
    "\n",
    "def load_mnist_torch(download_root: str = \"./data\", limit_train:int=None, limit_test:int=None, val_ratio:float=0.1):\n",
    "    \"\"\"\n",
    "    MNIST: 60k train, 10k test.\n",
    "    limit_train/test ile hızlı denemeler için örnek sayısını kısıtlayabilirsin (None = tümü).\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])  # 28x28 -> 784 flatten\n",
    "    train_ds_full = datasets.MNIST(download_root, train=True, transform=transform, download=True)\n",
    "    test_ds = datasets.MNIST(download_root, train=False, transform=transform, download=True)\n",
    "\n",
    "    if limit_train is not None:\n",
    "        train_ds_full = torch.utils.data.Subset(train_ds_full, list(range(min(limit_train, len(train_ds_full)))))\n",
    "    if limit_test is not None:\n",
    "        test_ds = torch.utils.data.Subset(test_ds, list(range(min(limit_test, len(test_ds)))))\n",
    "\n",
    "    # Train/Val böl\n",
    "    n_train = len(train_ds_full)\n",
    "    n_val = int(val_ratio * n_train)\n",
    "    n_tr = n_train - n_val\n",
    "    train_ds, val_ds = random_split(train_ds_full, [n_tr, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Paket boyutu: tensör dataset'e dönüştürelim (Subset zaten tensörleri içeriyor)\n",
    "    # Giriş boyutu 784, çıkış 10\n",
    "    input_size = 28*28\n",
    "    output_size = 10\n",
    "    return (train_ds, val_ds, test_ds), input_size, output_size\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Modeller\n",
    "# =========================================\n",
    "class ClassicANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden=[128, 64], output_size=10, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = input_size\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU(), nn.LayerNorm(h), nn.Dropout(pdrop)]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, output_size)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.apply(self._init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ONN_Bio(nn.Module):\n",
    "    \"\"\"\n",
    "    Basit MLP + biyolojik modüller:\n",
    "    - STDP-benzeri local plasticity (etkinlik korelasyonu ile weight gate güncelleme)\n",
    "    - Astrocyte modulation (aktivite düzeyine göre kazanç)\n",
    "    - Homeostatic scaling (katman aktivitesi hedef aralığa çekilir)\n",
    "    - Metabolic efficiency (L2 benzeri enerji reg.)\n",
    "    - Hafif osilasyon (sin modülasyonu)\n",
    "    Biyoloji kapatılabilir: bio_on=False => klasik MLP davranışı (aynı mimari)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden=[128, 64], output_size=10,\n",
    "                 pdrop=0.1,\n",
    "                 bio_on=True,\n",
    "                 plasticity=0.3,\n",
    "                 astrocyte_modulation=0.5,\n",
    "                 glia_neuron_interaction=0.5,\n",
    "                 oscillatory_pattern=0.3,\n",
    "                 homeostatic_scaling=1.0,\n",
    "                 metabolic_efficiency=0.0):\n",
    "        super().__init__()\n",
    "        self.bio_on = bio_on\n",
    "        self.plasticity = plasticity\n",
    "        self.astro = astrocyte_modulation\n",
    "        self.glia = glia_neuron_interaction\n",
    "        self.osc = oscillatory_pattern\n",
    "        self.homeo = homeostatic_scaling\n",
    "        self.metabolic = metabolic_efficiency\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.drops = nn.ModuleList()\n",
    "\n",
    "        sizes = [input_size] + hidden + [output_size]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if i < len(sizes)-1:  # hidden katmanlar için\n",
    "                if i < len(hidden):  # son linear (output) hariç\n",
    "                    self.norms.append(nn.LayerNorm(sizes[i+1]))\n",
    "                    self.drops.append(nn.Dropout(pdrop))\n",
    "\n",
    "        self.apply(self._init)\n",
    "        self.t = 0  # osilasyon zamanı\n",
    "        # Kapılayıcılar (gate) – ağırlık üzerine çarpımsal etki\n",
    "        self.register_buffer(\"weight_gate_0\", torch.ones(1))\n",
    "        self.weight_gates = nn.ParameterList()\n",
    "        for i in range(len(sizes)-1):\n",
    "            # her katman için ayrı gate vektörü (out_features boyutunda)\n",
    "            self.weight_gates.append(nn.Parameter(torch.ones(sizes[i+1]), requires_grad=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def _init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _bio_modulate(self, x, h_idx):\n",
    "        \"\"\"\n",
    "        Astrocyte + osilasyon + homeostatic gain modülasyonu.\n",
    "        h_idx: 0..len(hidden)-1\n",
    "        \"\"\"\n",
    "        if not self.bio_on or h_idx is None:\n",
    "            return x\n",
    "        # Astrocyte: aktivite tabanlı kazanç\n",
    "        gain = 1.0 + self.astro * torch.tanh(x.mean(dim=0, keepdim=True))  # (1, features)\n",
    "        # Osilasyon: zaman bazlı küçük sin modülasyonu\n",
    "        if self.osc != 0:\n",
    "            osc = 1.0 + 0.05 * math.sin(self.t * self.osc)\n",
    "            gain = gain * osc\n",
    "        # Homeostatic scaling: ortalama aktiviteyi hedefe çek\n",
    "        if self.homeo != 0:\n",
    "            target = 0.0\n",
    "            mean_act = x.mean().item()\n",
    "            gain = gain * (1.0 + 0.01 * self.homeo * (target - mean_act))\n",
    "        return x * gain\n",
    "\n",
    "    def _plasticity_update(self, pre, post, layer_idx):\n",
    "        \"\"\"\n",
    "        Basit STDP-benzeri: pre-post korelasyonuna göre kapı (gate) güncelle.\n",
    "        Gate değerleri ağırlığa çarpımsal etki yapar (effective W = W * gate).\n",
    "        \"\"\"\n",
    "        if (not self.bio_on) or self.plasticity == 0:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            # pre:(B, in), post:(B, out) -> out-düzeyinde gate\n",
    "            corr = torch.einsum(\"bi,bo->o\", pre, post) / (pre.size(0) + 1e-6)  # (out,)\n",
    "            corr = torch.tanh(corr)  # sınırlı etki\n",
    "            g = self.weight_gates[layer_idx]\n",
    "            g.data = torch.clamp(g.data + self.plasticity * 0.01 * corr, 0.5, 1.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.t += 1\n",
    "        h = x\n",
    "        pre = None\n",
    "        norm_i = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            W = layer.weight\n",
    "            b = layer.bias\n",
    "            # Kapı uygulama (glia etkileşimi ile güçlendirilebilir)\n",
    "            gate = self.weight_gates[i]\n",
    "            eff_W = W * gate.unsqueeze(1) * (1.0 + 0.1 * self.glia if self.bio_on else 1.0)\n",
    "            h = F.linear(h, eff_W, b)\n",
    "\n",
    "            is_hidden = i < (len(self.layers)-1)\n",
    "            if is_hidden:\n",
    "                h = F.gelu(h)\n",
    "                if self.bio_on:\n",
    "                    # astro & homeo & osc\n",
    "                    h = self._bio_modulate(h, h_idx=i)\n",
    "                # LayerNorm + Dropout\n",
    "                h = self.norms[norm_i](h)\n",
    "                h = self.drops[norm_i](h)\n",
    "                # plasticity update (pre, post)\n",
    "                if pre is not None:\n",
    "                    self._plasticity_update(pre, h, layer_idx=i)\n",
    "                pre = h.detach()\n",
    "                norm_i += 1\n",
    "\n",
    "        # Metabolic regularizer: forward’da ceza değeri döndürmek için hook yerine dışarıdan erişeceğiz\n",
    "        return h\n",
    "\n",
    "    def metabolic_penalty(self):\n",
    "        if not self.bio_on or self.metabolic == 0:\n",
    "            return 0.0\n",
    "        penalty = 0.0\n",
    "        for p in self.parameters():\n",
    "            if p.requires_grad and p.dim() >= 2:\n",
    "                penalty = penalty + p.pow(2).mean()\n",
    "        return self.metabolic * 1e-4 * penalty\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Eğitim & Değerlendirme\n",
    "# =========================================\n",
    "def train_one(model, train_loader, val_loader, device, epochs=10, lr=1e-3, weight_decay=0.0, patience=5):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).long()\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            # ONN metabolik ceza\n",
    "            if hasattr(model, \"metabolic_penalty\"):\n",
    "                pen = model.metabolic_penalty()\n",
    "                if isinstance(pen, torch.Tensor):\n",
    "                    loss = loss + pen\n",
    "                else:\n",
    "                    loss = loss + torch.tensor(pen, device=xb.device)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        val_accs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device).long()\n",
    "                logits = model(xb)\n",
    "                val_accs.append(accuracy_from_logits(logits, yb))\n",
    "        cur_val = float(np.mean(val_accs))\n",
    "        if cur_val > best_val:\n",
    "            best_val = cur_val\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def test_acc(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).long()\n",
    "            logits = model(xb)\n",
    "            accs.append(accuracy_from_logits(logits, yb))\n",
    "    return float(np.mean(accs))\n",
    "\n",
    "\n",
    "def run_dataset(name:str,\n",
    "                loader_fn,\n",
    "                input_size:int=None, output_size:int=None,\n",
    "                hidden=[128,64],\n",
    "                runs:int=5,\n",
    "                epochs_small:int=50,\n",
    "                epochs_mnist:int=5,\n",
    "                batch_small:int=32,\n",
    "                batch_mnist:int=128):\n",
    "    \"\"\"\n",
    "    Iris / Breast Cancer gibi küçük setler -> epochs_small\n",
    "    MNIST -> epochs_mnist (hız için)\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    ann_scores = []\n",
    "    onn_scores = []\n",
    "\n",
    "    # Veri\n",
    "    (train_ds, val_ds, test_ds), in_sz, out_sz = loader_fn()\n",
    "\n",
    "    # Loader seçimi\n",
    "    if name.lower().startswith(\"mnist\"):\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_mnist, shuffle=True, drop_last=False)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=batch_mnist, shuffle=False, drop_last=False)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=batch_mnist, shuffle=False, drop_last=False)\n",
    "        EPOCHS = epochs_mnist\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_small, shuffle=True, drop_last=False)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=batch_small, shuffle=False, drop_last=False)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=batch_small, shuffle=False, drop_last=False)\n",
    "        EPOCHS = epochs_small\n",
    "\n",
    "    for r in range(runs):\n",
    "        set_seed(42 + r)\n",
    "\n",
    "        # ANN\n",
    "        ann = ClassicANN(in_sz, hidden=hidden, output_size=out_sz, pdrop=0.1)\n",
    "        ann = train_one(ann, train_loader, val_loader, device, epochs=EPOCHS, lr=1e-3, weight_decay=1e-4, patience=8)\n",
    "        acc_ann = test_acc(ann, test_loader, device)\n",
    "        ann_scores.append(100.0 * acc_ann)\n",
    "\n",
    "        # ONN (bio açık)\n",
    "        onn = ONN_Bio(in_sz, hidden=hidden, output_size=out_sz,\n",
    "                      pdrop=0.1, bio_on=True,\n",
    "                      plasticity=0.3,                # stable varsayılanlar\n",
    "                      astrocyte_modulation=0.3,\n",
    "                      glia_neuron_interaction=0.3,\n",
    "                      oscillatory_pattern=0.1,\n",
    "                      homeostatic_scaling=0.5,\n",
    "                      metabolic_efficiency=0.0)\n",
    "        onn = train_one(onn, train_loader, val_loader, device, epochs=EPOCHS, lr=1e-3, weight_decay=1e-4, patience=8)\n",
    "        acc_onn = test_acc(onn, test_loader, device)\n",
    "        onn_scores.append(100.0 * acc_onn)\n",
    "\n",
    "    # İstatistik\n",
    "    stats = paired_stats(ann_scores, onn_scores)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Classic ANN: {np.mean(ann_scores):.2f}% ± {np.std(ann_scores, ddof=1):.2f}\")\n",
    "    print(f\"ONN_Bio:    {np.mean(onn_scores):.2f}% ± {np.std(onn_scores, ddof=1):.2f}\")\n",
    "    print(f\"P-value: {stats['p']:.4f}, Cohen's d(z): {stats['dz']:.3f}, 95% CI (ONN-ANN): [{stats['ci_low']:.2f}, {stats['ci_high']:.2f}]\")\n",
    "    return ann_scores, onn_scores, stats\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Çalıştır\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # MNIST indirme yolu\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "    # Veri yükleyiciler\n",
    "    def iris_loader():\n",
    "        return load_iris_torch()  # (train,val,test), in, out\n",
    "\n",
    "    def bc_loader():\n",
    "        return load_breast_cancer_torch()\n",
    "\n",
    "    def mnist_loader():\n",
    "        # Hızlı deneme için limitleri None bırak (tam 60k/10k). Hız istersen limit_train=20000, limit_test=5000 yap.\n",
    "        return load_mnist_torch(download_root=\"./data\", limit_train=None, limit_test=None, val_ratio=0.1)\n",
    "\n",
    "    # Koşular\n",
    "    iris_ann, iris_onn, iris_stats = run_dataset(\"Iris\", iris_loader, hidden=[64, 32], runs=5, epochs_small=60)\n",
    "    bc_ann, bc_onn, bc_stats = run_dataset(\"Breast Cancer\", bc_loader, hidden=[128, 64], runs=5, epochs_small=60)\n",
    "    mnist_ann, mnist_onn, mnist_stats = run_dataset(\"MNIST\", mnist_loader, hidden=[256, 128], runs=3, epochs_mnist=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc16bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
